{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T03:37:43.095383Z",
     "start_time": "2025-03-24T03:37:30.927222Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/DL_learning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "----------------------------------------\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from webencodings import labels\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "input_text = \"Transformer are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iters = []\n",
    "n_step3 = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_step3):\n",
    "        iter = dict()\n",
    "        iter[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # store tokens with highest probs\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            iter[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "            \n",
    "            # append predicted next token to input\n",
    "            input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iters.append(iter)\n",
    "\n",
    "df = pd.DataFrame(iters)\n",
    "df.to_html(\"/Users/admin/Developer/PyProjects/DeepLearing/output/output.html\")\n",
    "\n",
    "#use generate\n",
    "output = model.generate(input_ids, do_sample=False, max_new_tokens=n_step3)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"-\"*40)\n",
    "#reproduce unicorn stories\n",
    "max_length = 128\n",
    "input_text = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d874f10df58f2",
   "metadata": {},
   "source": [
    "## Greedy Search vs Beam Search Decoding vs Stochastic Beam Search\n",
    "### Greedy Search （贪心搜索）\n",
    "* 每一步都选择概率最高的一个单词作为下一个token，只跟踪一条最优路径，不考虑其他选项。\n",
    "* 公式：$ Y_{t+1} = \\arg \\max_{ w}P(w \\mid Y_{1:t}) $ <br><br>\n",
    "* 计算快实现简单，但容易卡在局部最优，因为他不考虑未来选项，且缺乏多样性，相同输入总是相同输出。<br>\n",
    "\n",
    "### Beam Search Decoding（集束搜索）\n",
    "* 维护 k 条可能的路径（beam size = k），在每一步选择 k 个最可能的序列继续扩展，而不是只选一个最优路径。\n",
    "* 最终返回概率最高的完整句子。\n",
    "* 公式：近似最大化序列概率 $ Y^* = \\arg \\max_{ y}P(Y \\mid X) $\n",
    "1. 设 beam_size = k（比如 k=3）\n",
    "2. 在每一步，计算所有扩展路径的概率，保留 k 个最优的序列。\n",
    "3. 直到句子结束符 <EOS>，选择最高概率的完整序列。\n",
    "<br>\n",
    "比贪心搜索更优，避免局部最优解。可调控 k 值，在质量和计算成本之间取得平衡。但 计算开销较大，k 越大，计算量越高；仍然可能丢失全局最优解，如果 k 过小，搜索空间不够大。\n",
    "\n",
    "### Stochastic Beam Search（随机集束搜索）\n",
    "\n",
    "* 传统 Beam Search 只保留最高概率的路径，而 Stochastic Beam Search 会随机选择 k 个候选路径。\n",
    "* 引入随机性，避免所有生成的文本都完全相同，提升文本多样性\n",
    "* 过程\n",
    "1. 采样候选路径，而不是每次都选 top-k 的最优路径。\n",
    "2. 在 beam_size 内，使用温度系数调整概率：\n",
    "$ P(w) \\propto P(w)^{1/T} $ <br>\n",
    "    T > 1 → 选择更随机的词 <br>\n",
    "    T < 1 → 选择更确定的词 <br>\n",
    "\n",
    "生成更多样化的文本，适用于对话、故事生成等任务。不会死板地选取最优解，更像人类写作风格。<br>\n",
    "质量不一定最优，可能选到概率较低的词。需要调节超参数，如 T 和 k，以保证质量。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5a811496b87f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T03:38:16.723579Z",
     "start_time": "2025-03-24T03:38:06.182060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "Sequence log probs: -2.31\n",
      "-bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs-\n",
      "beam search:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "log-prob: -2.31\n",
      "-bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng-\n",
      "\n",
      "\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "The most powerful weapon in the game. The most powerful weapons in the world.\n",
      "\n",
      "This is a list of all the most powerful and powerful weapon weapons that can be found in the World of Warcraft: The Burning Crusade. The list is not exhaustive, and may change at any time. For more information, see the list of known and unknown weapons.\n",
      "\n",
      "\n",
      "Note: This list is based on the official\n",
      "\n",
      "log-prob: -114.80\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    single token log probability from logits\n",
    "    :param logits: \n",
    "    :param labels: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprobs(model, labels, input_len = 0):\n",
    "    \"\"\"\n",
    "    sequence log probability\n",
    "    :param model: \n",
    "    :param labels: \n",
    "    :param input_len: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_probs = torch.sum(log_probs[:, input_len: ])\n",
    "    return seq_log_probs\n",
    "\n",
    "logp = sequence_logprobs(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(\"-/\"*20)\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"Sequence log probs: {logp:.2f}\")\n",
    "\n",
    "print(\"-bs-\"*20)\n",
    "print('beam search:\\n')\n",
    "# beam search\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    " do_sample=False)\n",
    "logp = sequence_logprobs(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n",
    "\n",
    "# beam search with ngram\n",
    "print(\"-bs ng-\"*20)\n",
    "print(\"\\n\")\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    " do_sample=False, no_repeat_ngram_size=3)\n",
    "logp = sequence_logprobs(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474f4dca8558ab8",
   "metadata": {},
   "source": [
    "> <strong> N-gram Penalty（N-gram 惩罚）</strong>  : N-gram Penalty（N-gram 惩罚）是一种 解码策略，用于 防止重复生成相同的 N-gram 片段，尤其在 文本生成任务（如机器翻译、文本摘要、对话系统）中，能有效减少 重复问题。<br>\n",
    "> N-gram 指的是由 N 个连续 token 组成的短语\n",
    "> 重复模式 发生的原因: <br>\n",
    ">   a. 语言模型学习到了某些短语的高概率，导致它 倾向于重复生成。 <br>\n",
    ">   b. 在 Beam Search 或 Greedy Search 过程中，较高概率的 token 会不断被选中，形成重复。<br>\n",
    "> How it works? <br>\n",
    ">   1. 记录 已经生成的 N-gram 片段 <br>\n",
    ">   2. 当解码过程中尝试生成下一个 token 时: 如果这个 token 形成了已存在的 N-gram，则 禁止生成（分数设为极低）。这样就可以 避免完全相同的重复短语。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945ebdd814a23b8",
   "metadata": {},
   "source": [
    "## Sampling Methods\n",
    "\n",
    "Reduces repetitions.\n",
    "\n",
    "### 随机采样\n",
    "最简单的采样方法是在每个时间步随机从模型输出的整个词汇表的概率分布中进行采样。 <br>\n",
    "\n",
    "$P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\text{softmax}(z_{t,i}) = \\frac{\\exp(z_{t,i})}{\\sum_{j=1}^{|V|} \\exp(z_{t,j})} $ <br>\n",
    "\n",
    "V 表示词表基数，可以通过增加一个温度参数 T在softmax操作之前缩放logits，以此控制输出结果的多样性。<br>\n",
    "\n",
    "$P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\frac{\\exp(z_{t,i} / T)}{\\sum_{j=1}^{|V|} \\exp(z_{t,j} / T)}$ <br><br>\n",
    "\n",
    "调整T来控制概率分布的形状。T<= 1, 分布在原点附近变得尖锐，稀有的token被抑制；当温度 T≫1 时，分布变得平坦，每个token变得同样可能。 <br><br>\n",
    "<img src=\"./imgs/temperature_contrl_logits_scale.png\" width=\"400\"/> <br>\n",
    "\n",
    "* T 越小，概率分布越陡峭，输出更确定（接近贪心解）。\n",
    "* T 越大，概率分布越平缓，采样更随机（提升多样性但可能导致错误）。\n",
    "* 通常T 设为 0.7 ~ 1.0 之间，以平衡确定性与多样性\n",
    "\n",
    "### Top-K and Nucleus Sampling\n",
    "\n",
    "核心思想是限制参与采样的相关token数量。Top-k 取K个最高概率的token。避免选择低概率token作为下一个输出token。相比于Top-k的固定阶段方式，Top-p采用动态截断，设定一个阈值，比如95%，\n",
    "将所有token按概率值降序，以此高->低取，并逐个累加概率和直至达到阈值95%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f4c7358d5392f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T06:07:59.244156Z",
     "start_time": "2025-03-24T06:07:56.308001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k-\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "log-prob: -2.31\n",
      "-top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p-\n",
      "\n",
      "\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "log-prob: -2.31\n"
     ]
    }
   ],
   "source": [
    "print(\"-top-k-\"*20)\n",
    "print(\"\\n\")\n",
    "output_topk = model.generate(input_ids, max_length=max_length,\n",
    " do_sample=True, top_k=50)\n",
    "logp = sequence_logprobs(model, output_topk, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_topk[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n",
    "\n",
    "print(\"-top-p-\"*20)\n",
    "print(\"\\n\")\n",
    "output_topp = model.generate(input_ids, max_length=max_length,\n",
    " do_sample=True, top_p=0.6)\n",
    "logp = sequence_logprobs(model, output_topp, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_topp[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f662c62c139c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Summarization\n",
    "\n",
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabbe49d1cf5561e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T08:15:07.421201Z",
     "start_time": "2025-03-24T08:15:01.700812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:['article', 'highlights', 'id']\n",
      "\n",
      "Article (excerpt of 500 characters, total length:\n",
      "4051):\n",
      "\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      "Summary (length: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(f\"Features:{dataset['train'].column_names}\")\n",
    "sample = dataset['train'][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length:\n",
    "{len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700202d793b67",
   "metadata": {},
   "source": [
    "处理缩写中的标点符号，如：U.S. or U.N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31253d7fd8d44625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:39:39.504626Z",
     "start_time": "2025-03-24T10:39:39.489312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/admin/nltk_data', '/Users/admin/miniconda3/envs/DL_learning/nltk_data', '/Users/admin/miniconda3/envs/DL_learning/share/nltk_data', '/Users/admin/miniconda3/envs/DL_learning/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/admin/nltk_data/', '/Users/admin/']\n",
      "Python Version: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:43:39) [Clang 11.1.0 ]\n",
      "NLTK Version: 3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['铁木真.阿济格.', 'The U.N. is an organization.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(nltk.data.path)\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NLTK Version: {nltk.__version__}\")\n",
    "\n",
    "nltk.download('punkt_tab', download_dir='/Users/admin/nltk_data')\n",
    "string = \"铁木真.阿济格. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd920aa1bcacd2bc",
   "metadata": {},
   "source": [
    "### Summarization Baseline\n",
    "比较常用的一个方式是直接取文章的前三个句子。以下是用NLTK实现baseline的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe29474053121b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-24T10:49:33.098515Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/DL_learning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "summaries = {}\n",
    "\n",
    "## baseline\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)\n",
    "\n",
    "## 使用GPT2做摘要任务\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_output = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_output[0][\"generated_text\"][len(gpt2_query):]))\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b9e7fde7ae390",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
