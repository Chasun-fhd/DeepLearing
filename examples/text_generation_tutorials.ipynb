{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T03:37:43.095383Z",
     "start_time": "2025-03-24T03:37:30.927222Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from webencodings import labels\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "input_text = \"Transformer are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iters = []\n",
    "n_step3 = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_step3):\n",
    "        iter = dict()\n",
    "        iter[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # store tokens with highest probs\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            iter[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "            \n",
    "            # append predicted next token to input\n",
    "            input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iters.append(iter)\n",
    "\n",
    "df = pd.DataFrame(iters)\n",
    "df.to_html(\"/Users/admin/Developer/PyProjects/DeepLearing/output/output.html\")\n",
    "\n",
    "#use generate\n",
    "output = model.generate(input_ids, do_sample=False, max_new_tokens=n_step3)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"-\"*40)\n",
    "#reproduce unicorn stories\n",
    "max_length = 128\n",
    "input_text = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/DL_learning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "----------------------------------------\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Greedy Search vs Beam Search Decoding vs Stochastic Beam Search\n",
    "### Greedy Search （贪心搜索）\n",
    "* 每一步都选择概率最高的一个单词作为下一个token，只跟踪一条最优路径，不考虑其他选项。\n",
    "* 公式：$ Y_{t+1} = \\arg \\max_{ w}P(w \\mid Y_{1:t}) $ <br><br>\n",
    "* 计算快实现简单，但容易卡在局部最优，因为他不考虑未来选项，且缺乏多样性，相同输入总是相同输出。<br>\n",
    "\n",
    "### Beam Search Decoding（集束搜索）\n",
    "* 维护 k 条可能的路径（beam size = k），在每一步选择 k 个最可能的序列继续扩展，而不是只选一个最优路径。\n",
    "* 最终返回概率最高的完整句子。\n",
    "* 公式：近似最大化序列概率 $ Y^* = \\arg \\max_{ y}P(Y \\mid X) $\n",
    "1. 设 beam_size = k（比如 k=3）\n",
    "2. 在每一步，计算所有扩展路径的概率，保留 k 个最优的序列。\n",
    "3. 直到句子结束符 <EOS>，选择最高概率的完整序列。\n",
    "<br>\n",
    "比贪心搜索更优，避免局部最优解。可调控 k 值，在质量和计算成本之间取得平衡。但 计算开销较大，k 越大，计算量越高；仍然可能丢失全局最优解，如果 k 过小，搜索空间不够大。\n",
    "\n",
    "### Stochastic Beam Search（随机集束搜索）\n",
    "\n",
    "* 传统 Beam Search 只保留最高概率的路径，而 Stochastic Beam Search 会随机选择 k 个候选路径。\n",
    "* 引入随机性，避免所有生成的文本都完全相同，提升文本多样性\n",
    "* 过程\n",
    "1. 采样候选路径，而不是每次都选 top-k 的最优路径。\n",
    "2. 在 beam_size 内，使用温度系数调整概率：\n",
    "$ P(w) \\propto P(w)^{1/T} $ <br>\n",
    "    T > 1 → 选择更随机的词 <br>\n",
    "    T < 1 → 选择更确定的词 <br>\n",
    "\n",
    "生成更多样化的文本，适用于对话、故事生成等任务。不会死板地选取最优解，更像人类写作风格。<br>\n",
    "质量不一定最优，可能选到概率较低的词。需要调节超参数，如 T 和 k，以保证质量。<br>"
   ],
   "id": "192d874f10df58f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T03:38:16.723579Z",
     "start_time": "2025-03-24T03:38:06.182060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    single token log probability from logits\n",
    "    :param logits: \n",
    "    :param labels: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprobs(model, labels, input_len = 0):\n",
    "    \"\"\"\n",
    "    sequence log probability\n",
    "    :param model: \n",
    "    :param labels: \n",
    "    :param input_len: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_probs = torch.sum(log_probs[:, input_len: ])\n",
    "    return seq_log_probs\n",
    "\n",
    "logp = sequence_logprobs(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(\"-/\"*20)\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"Sequence log probs: {logp:.2f}\")\n",
    "\n",
    "print(\"-bs-\"*20)\n",
    "print('beam search:\\n')\n",
    "# beam search\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    " do_sample=False)\n",
    "logp = sequence_logprobs(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n",
    "\n",
    "# beam search with ngram\n",
    "print(\"-bs ng-\"*20)\n",
    "print(\"\\n\")\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    " do_sample=False, no_repeat_ngram_size=3)\n",
    "logp = sequence_logprobs(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n"
   ],
   "id": "7b5a811496b87f70",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "Sequence log probs: -2.31\n",
      "-bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs--bs-\n",
      "beam search:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "log-prob: -2.31\n",
      "-bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng--bs ng-\n",
      "\n",
      "\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "The most powerful weapon in the game. The most powerful weapons in the world.\n",
      "\n",
      "This is a list of all the most powerful and powerful weapon weapons that can be found in the World of Warcraft: The Burning Crusade. The list is not exhaustive, and may change at any time. For more information, see the list of known and unknown weapons.\n",
      "\n",
      "\n",
      "Note: This list is based on the official\n",
      "\n",
      "log-prob: -114.80\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> <strong> N-gram Penalty（N-gram 惩罚）</strong>  : N-gram Penalty（N-gram 惩罚）是一种 解码策略，用于 防止重复生成相同的 N-gram 片段，尤其在 文本生成任务（如机器翻译、文本摘要、对话系统）中，能有效减少 重复问题。<br>\n",
    "> N-gram 指的是由 N 个连续 token 组成的短语\n",
    "> 重复模式 发生的原因: <br>\n",
    ">   a. 语言模型学习到了某些短语的高概率，导致它 倾向于重复生成。 <br>\n",
    ">   b. 在 Beam Search 或 Greedy Search 过程中，较高概率的 token 会不断被选中，形成重复。<br>\n",
    "> How it works? <br>\n",
    ">   1. 记录 已经生成的 N-gram 片段 <br>\n",
    ">   2. 当解码过程中尝试生成下一个 token 时: 如果这个 token 形成了已存在的 N-gram，则 禁止生成（分数设为极低）。这样就可以 避免完全相同的重复短语。<br>"
   ],
   "id": "7474f4dca8558ab8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sampling Methods\n",
    "\n",
    "Reduces repetitions.\n",
    "\n",
    "### 随机采样\n",
    "最简单的采样方法是在每个时间步随机从模型输出的整个词汇表的概率分布中进行采样。 <br>\n",
    "\n",
    "$P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\text{softmax}(z_{t,i}) = \\frac{\\exp(z_{t,i})}{\\sum_{j=1}^{|V|} \\exp(z_{t,j})} $ <br>\n",
    "\n",
    "V 表示词表基数，可以通过增加一个温度参数 T在softmax操作之前缩放logits，以此控制输出结果的多样性。<br>\n",
    "\n",
    "$P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\frac{\\exp(z_{t,i} / T)}{\\sum_{j=1}^{|V|} \\exp(z_{t,j} / T)}$ <br><br>\n",
    "\n",
    "调整T来控制概率分布的形状。T<= 1, 分布在原点附近变得尖锐，稀有的token被抑制；当温度 T≫1 时，分布变得平坦，每个token变得同样可能。 <br><br>\n",
    "<img src=\"./imgs/temperature_contrl_logits_scale.png\" width=\"400\"/> <br>\n",
    "\n",
    "* T 越小，概率分布越陡峭，输出更确定（接近贪心解）。\n",
    "* T 越大，概率分布越平缓，采样更随机（提升多样性但可能导致错误）。\n",
    "* 通常T 设为 0.7 ~ 1.0 之间，以平衡确定性与多样性\n",
    "\n",
    "### Top-K and Nucleus Sampling\n",
    "\n",
    "核心思想是限制参与采样的相关token数量。Top-k 取K个最高概率的token。避免选择低概率token作为下一个输出token。相比于Top-k的固定阶段方式，Top-p采用动态截断，设定一个阈值，比如95%，\n",
    "将所有token按概率值降序，以此高->低取，并逐个累加概率和直至达到阈值95%。"
   ],
   "id": "b945ebdd814a23b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T06:07:59.244156Z",
     "start_time": "2025-03-24T06:07:56.308001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"-top-k-\"*20)\n",
    "print(\"\\n\")\n",
    "output_topk = model.generate(input_ids, max_length=max_length,\n",
    " do_sample=True, top_k=50)\n",
    "logp = sequence_logprobs(model, output_topk, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_topk[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n",
    "\n",
    "print(\"-top-p-\"*20)\n",
    "print(\"\\n\")\n",
    "output_topp = model.generate(input_ids, max_length=max_length,\n",
    " do_sample=True, top_p=0.6)\n",
    "logp = sequence_logprobs(model, output_topp, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_topp[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ],
   "id": "d8f4c7358d5392f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k--top-k-\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "log-prob: -2.31\n",
      "-top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p--top-p-\n",
      "\n",
      "\n",
      "Transformer are the most most most most most powerful powerful powerful powerful powerful weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon weapon\n",
      "\n",
      "log-prob: -2.31\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Text Summarization\n",
    "\n",
    "### Datasets"
   ],
   "id": "e5f662c62c139c57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T08:15:07.421201Z",
     "start_time": "2025-03-24T08:15:01.700812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(f\"Features:{dataset['train'].column_names}\")\n",
    "sample = dataset['train'][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length:\n",
    "{len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ],
   "id": "eabbe49d1cf5561e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:['article', 'highlights', 'id']\n",
      "\n",
      "Article (excerpt of 500 characters, total length:\n",
      "4051):\n",
      "\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      "Summary (length: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "处理缩写中的标点符号，如：U.S. or U.N.",
   "id": "b700202d793b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:39:39.504626Z",
     "start_time": "2025-03-24T10:39:39.489312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(nltk.data.path)\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NLTK Version: {nltk.__version__}\")\n",
    "\n",
    "nltk.download('punkt_tab', download_dir='/Users/admin/nltk_data')\n",
    "string = \"铁木真.阿济格. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ],
   "id": "31253d7fd8d44625",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/admin/nltk_data', '/Users/admin/miniconda3/envs/DL_learning/nltk_data', '/Users/admin/miniconda3/envs/DL_learning/share/nltk_data', '/Users/admin/miniconda3/envs/DL_learning/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/admin/nltk_data/', '/Users/admin/']\n",
      "Python Version: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:43:39) [Clang 11.1.0 ]\n",
      "NLTK Version: 3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['铁木真.阿济格.', 'The U.N. is an organization.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summarization Baseline\n",
    "比较常用的一个方式是直接取文章的前三个句子。以下是用NLTK实现baseline的例子："
   ],
   "id": "dd920aa1bcacd2bc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-25T09:29:04.013299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "summaries = {}\n",
    "\n",
    "## baseline\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)\n",
    "\n",
    "## 使用GPT2做摘要任务\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_output = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_output[0][\"generated_text\"][len(gpt2_query):]))\n",
    "print(summaries)\n",
    "\n",
    "#use t5: 通过将所有任务形式化为文本到文本的任务来创建一个通用的transformer 架构; T5检查点是在无监督数据（用于重建遮蔽词）和多个任务的监督数据（包括摘要）的混合数据上进行训练的。\n",
    "# pipe = pipeline(\"summarization\", model=\"t5-small\")\n",
    "# pipe_out = pipe(sample_text, num_beams=2, max_length=50)\n",
    "# summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "# print(summaries)\n",
    "\n",
    "#use bart: encoder->decoder架构，被训练用于重构受损的输入，结合了gpt2与bert的预训练方案"
   ],
   "id": "bbfe29474053121b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics\n",
    "生成式语言模型能力评估常用的两种方式：BLEU ROUGE\n",
    "\n",
    "### BLEU （Bilingual Evaluation Understudy）\n",
    "关注词汇或者n-gram，而不再只是关注生成文本中多少token与参考文本的token对齐。BLEU是一种基于精确度的度量标准，在比较两段文本时，统计生成的文本有多少词是在参考文本里面，再除以参考文本长度，从而得出精确度。<br>\n",
    "缺陷：如果生成文本中有重复词汇，且这个词汇又在参考文本中出现，那么上述方案计算出的精确度就不置信 <br>\n",
    "解决：统计生成文本中的词汇出现次数时，以参考文本中该词汇出现次数作为统计上限。<br><br>\n",
    "a. **n-gram精确度计算**<br>\n",
    "$P_n=\\sum\\frac{候选文本中匹配的_{n-gram}}{候选文本中所有的_{n-gram}}$ <br><br>\n",
    "\n",
    "b. **BP（Brevity Penalty) 长度惩罚**<br>\n",
    "如生成的句子比参考文本短很多，会导致BLEU过高，引入惩罚项即可：<br>\n",
    "$BP=\\begin{cases}\n",
    "1, & \\text{if } c > r\\\\\n",
    "e^{1-r/c}, & \\text{if } c \\leq r\n",
    "\\end{cases}\n",
    "$\n",
    "* c=生成文本长度\n",
    "* r=参考文本长度\n",
    "\n",
    "c.**BLEU 计算公式**\n",
    "$BLEU=BP.exp \\left( \\sum^{N}_{n=1}{w_n logP_n} \\right)$\n",
    "### ROUGE\n",
    "\n",
    "计算生成文本和参考文本之间的 n-gram 重叠情况。主要关注 召回率（Recall），即参考文本中的内容有多少被生成文本包含。\n",
    "\n",
    "常见变体：\n",
    "1. ROUGE-N（n-gram召回率）\n",
    "$ROUGE-N=\\frac {\\sum{匹配的_{n-gram}数}}{\\sum{参考文本的_{n-gram}总数}}$\n",
    "\n",
    "2. ROUGE-L 最长公共子序列(LCS)\n",
    "计算生成文本与参考文本之间的最长公共子序列（LCS），避免 n-gram 过于严格。\n",
    "$ROUGE-L=\\frac{LCS长度}{参考文本长度}$\n",
    "> \n",
    "> 参考：\"The cat is on the mat.\" \n",
    "> \n",
    "> 生成：\"The cat is on mat.\"\n",
    "> \n",
    "> LCS = \"The cat is on mat.\"（匹配长度 5）\n",
    "> \n",
    "> $ROUGE-L = \\frac{5}{6}$ \n",
    "\n",
    "3. ROUGE-W（加权最长公共子序列）\n",
    "ROUGE-L 改进版本，给较长的LCS更高权重\n",
    "\n",
    "4. ROUGE-S（跳跃bi-gram统计）\n",
    "统计允许跳跃的二元组匹配，计算非连续的短语重叠率\n",
    "\n",
    "d.**BLEU与ROUGE区别** \n",
    "<br><br>\n",
    "<img src=\"./imgs/bleu_vs_rouge.png\" width=\"400\"/>"
   ],
   "id": "d40b9e7fde7ae390"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e19c076adbf29f48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T09:25:45.930922Z",
     "start_time": "2025-03-25T09:25:31.004583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用nltk库计算BLEU score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]\n",
    "candidate = [\"the\", \"cat\", \"is\", \"on\", \"mat\"]\n",
    "\n",
    "#BLUE-1 ~ 4\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(f\"BLEU score: {score:.4f}\")\n",
    "\n",
    "#使用evaluate\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "bleu_metric.add(prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in\n",
    "results[\"precisions\"]]\n",
    "df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\n",
    "print(df)\n",
    "print(\"-\"*30)\n",
    "bleu_metric.add(prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in\n",
    "results[\"precisions\"]]\n",
    "df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\n",
    "print(df)\n",
    "\n",
    "# 使用rouge-score库计算rouge召回率\n",
    "from rouge_score import rouge_scorer\n",
    "from datasets import load_dataset\n",
    "reference = \"The cat is on the mat.\"\n",
    "candidate = \"The cat is on mat.\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(f\"ROUGE-1 score: {scores['rouge1'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-2 score: {scores['rouge2'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-L score: {scores['rougeL'].fmeasure:.4f}\")\n",
    "\n",
    "# 使用evaluate计算rouge\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "for model_name in summaries:\n",
    " rouge_metric.add(prediction=summaries[model_name],\n",
    "reference=reference)\n",
    " score = rouge_metric.compute()\n",
    " rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in\n",
    "rouge_names)\n",
    " records.append(rouge_dict)\n",
    "df = pd.DataFrame.from_records(records, index=summaries.keys())\n",
    "print(df)"
   ],
   "id": "c1cf9a44942ca40f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.5789\n",
      "                             Value\n",
      "score                          0.0\n",
      "counts                [2, 0, 0, 0]\n",
      "totals                [6, 5, 4, 3]\n",
      "precisions  [33.33, 0.0, 0.0, 0.0]\n",
      "bp                             1.0\n",
      "sys_len                          6\n",
      "ref_len                          6\n",
      "------------------------------\n",
      "                                 Value\n",
      "score                        57.893007\n",
      "counts                    [5, 3, 2, 1]\n",
      "totals                    [5, 4, 3, 2]\n",
      "precisions  [100.0, 75.0, 66.67, 50.0]\n",
      "bp                            0.818731\n",
      "sys_len                              5\n",
      "ref_len                              6\n",
      "ROUGE-1 score: 0.9091\n",
      "ROUGE-2 score: 0.6667\n",
      "ROUGE-L score: 0.9091\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summaries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 49\u001B[0m\n\u001B[1;32m     47\u001B[0m records \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     48\u001B[0m rouge_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrouge1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrouge2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrougeL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrougeLsum\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m---> 49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name \u001B[38;5;129;01min\u001B[39;00m \u001B[43msummaries\u001B[49m:\n\u001B[1;32m     50\u001B[0m  rouge_metric\u001B[38;5;241m.\u001B[39madd(prediction\u001B[38;5;241m=\u001B[39msummaries[model_name],\n\u001B[1;32m     51\u001B[0m reference\u001B[38;5;241m=\u001B[39mreference)\n\u001B[1;32m     52\u001B[0m  score \u001B[38;5;241m=\u001B[39m rouge_metric\u001B[38;5;241m.\u001B[39mcompute()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'summaries' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train own summarization model\n",
    "\n",
    "`see python/train_own_summarization_model.py`"
   ],
   "id": "885669829c75bea2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
