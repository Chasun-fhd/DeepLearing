{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers inference acceleration\n",
    "\n",
    "> 以基于bert的意图识别模型为例"
   ],
   "id": "50bf519ea9434818"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T01:55:17.638042Z",
     "start_time": "2025-04-22T01:52:27.038153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from time import perf_counter\n",
    "from datasets import load_dataset\n",
    "import evaluate, torch\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
    "\n",
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th\n",
    "in\n",
    "Paris and I need a 15 passenger van\"\"\"\n",
    "result = pipe(query)\n",
    "print(result)\n",
    "\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERTbaseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset:\n",
    "            pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "        label = example[\"intent\"]\n",
    "        preds.append(intents.str2int(pred))\n",
    "        labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions=preds,\n",
    "                                          references=labels)\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "\n",
    "    def compute_size(self):\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        # Calculate size in megabytes\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        # Delete temporary file\n",
    "        tmp_path.unlink()\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def time_pipeline(self, query=\"What is the pin number for myaccount?\"):\n",
    "        latencies = []\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = self.pipeline(query)\n",
    "        # Timed run\n",
    "        for _ in range(100):\n",
    "            start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "        # Compute run statistics\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\-{time_std_ms: .2f}\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "\n",
    "sample = clinc[\"test\"][42]\n",
    "print(sample)\n",
    "\n",
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"])\n",
    "\n",
    "accuracy_score = evaluate.load(\"accuracy\")\n",
    "print(accuracy_score)\n",
    "\n",
    "list(pipe.model.state_dict().items())[42]\n",
    "# torch.save(pipe.model.state_dict(), \"model.pt\")\n",
    "\n",
    "pb = PerformanceBenchmark(pipe, clinc['test'])\n",
    "perf_metrics = pb.run_benchmark()\n",
    "print(perf_metrics)"
   ],
   "id": "9351d6be44275761",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'car_rental', 'score': 0.5490034818649292}]\n",
      "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}\n",
      "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n",
      "\"\"\", stored examples: 0)\n",
      "Model size (MB) - 418.15\n",
      "Average latency (ms) - 29.33 +\\- 0.00\n",
      "Accuracy on test set - 0.000\n",
      "{'BERTbaseline': {'size_mb': 418.1471004486084, 'time_avg_ms': 29.32579199978136, 'time_std_ms': 0.0, 'accuracy': 0.0}}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T13:45:40.713117Z",
     "start_time": "2025-04-21T13:45:30.984630Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "id": "faa5a27648508646",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a4/62/92e9cec3deca8b45abf62dd8f6469d688b3f28b9c170809fcc46f110b523/scikit_learn-1.3.2-cp38-cp38-macosx_12_0_arm64.whl (9.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.4/9.4 MB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /Users/admin/Developer/PyProjects/DeepLearing/.venv/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\r\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/93/4a/50c436de1353cce8b66b26e49a687f10b91fe7465bf34e4565d810153003/scipy-1.10.1-cp38-cp38-macosx_12_0_arm64.whl (28.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m28.8/28.8 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting joblib>=1.1.1 (from scikit-learn)\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\r\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.5.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Knowledge distillation 知识蒸馏\n",
    "\n",
    "核心思路是：在真实标签的基础上，再加上一份来自教师模型的“软概率分布”。这份分布可以给学生模型提供额外的信息。\n",
    "\n",
    "### Knowledge distillation for Fine-Tuning\n",
    "\n",
    "举个例子，比如我们有个 BERT-base 分类器（老师），它对多个意图都给出了较高的概率。这可能说明这些意图在特征空间中彼此靠得很近。那我们就可以训练学生模型去“模仿”这些概率分布。\n",
    "\n",
    "通过这种方式，我们希望把老师模型里学到的一些**“暗知识”（dark knowledge）**提炼出来给学生。所谓“暗知识”就是那些单靠真实标签是学不到的信息。\n",
    "<br/><br/>\n",
    "<img src=\"./imgs/student_teacher_soften_max.png\" width=\"400\"/>\n",
    "<br/><br/>\n",
    "\n",
    "*KL(Kullback–Leibler)* : 衡量学生老师模型分别生成的logits 概率差异，计算公式如下：\n",
    "<br/><br/>\n",
    "\n",
    "$D_KL(p, q) = \\sum_{i}pi(x)log\\frac{p_i(x)}{q_i(x)}$\n",
    "\n",
    "<br/><br/>可用于定义知识蒸馏的loss <br/><br/>\n",
    "$L_{KD} = T^2D_{KL}$\n",
    "\n",
    "在知识蒸馏里我们会用温度 T 来“软化”老师模型的输出概率，但这样会让梯度变小（大概变成原来的 1/T²），所以我们引入一个 T² 的归一化系数来把梯度调回合适的范围。温度 T 越高，\n",
    "softmax 输出的概率分布就越“平”，反之越尖锐，而这个平滑度直接影响训练时梯度的大小，所以引入 T² 是为了让蒸馏过程中的训练更稳定有效。\n",
    "\n",
    "## Quantization 量化\n",
    "\n",
    "## Pruning 剪枝\n",
    "\n",
    "## Graph optimization 图优化"
   ],
   "id": "4ce8af16cbe44277"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
